\documentclass[fleqn,leqno]{article}

% Defining page margins
\usepackage[top=1in, bottom=1.5in, left=1in, right=1in]{geometry}

% Ability to generate urls
\usepackage{hyperref}

\usepackage{mathtools}
\usepackage{amsmath}

% Pretty printing of references to multiple equations
\usepackage{cleveref}
\newcommand{\crefrangeconjunction}{--}

% argmin function
\DeclareMathOperator*{\argmin}{arg\,min}

% http://tex.stackexchange.com/questions/118410/highlight-terms-in-equation-mode
\usepackage{xcolor}
\newcommand{\highlight}[1]{\colorbox{green!10}{$\displaystyle#1$}}
\newcommand{\highlightred}[1]{\colorbox{olive!10}{$\displaystyle#1$}}

\begin{document}

\title{Detailed derivation of Belief Propagation algorithm \\ on Pairwise Markov Random Fields}
\author{Yurii Lahodiuk \\ yura.lagodiuk@gmail.com}
\date{}
\maketitle

By definition of probability density function:
\begin{equation} \label{eq:probability_sum}
\sum_{x_1} \sum_{x_2} \cdots \sum_{x_n}  p(x_1, x_2, \dots x_n) = 1
\end{equation}

We are interested in \lq \lq single-node\rq \rq\ marginal probabilities:
\begin{equation} \label{eq:marginal_probabilities_one_node}
\begin{split}
p_i(x_i) = \sum_{x_1} \cdots \sum_{ \highlight{ x_{i - 1} } } \sum_{ \highlight{ x_{i + 1} } } \cdots \sum_{x_n} p(x_1, x_2, \dots x_n) \\
p_j(x_i) = \sum_{x_1} \cdots \sum_{ \highlight{ x_{j - 1} } } \sum_{ \highlight{ x_{j + 1} } } \cdots \sum_{x_n} p(x_1, x_2, \dots x_n) \\
\end{split}
\end{equation}

For nodes, which are connected -- we, also interested in \lq \lq pairwise\rq \rq\ marginal probabilities:
\begin{equation} \label{eq:marginal_probabilities_two_nodes}
\begin{split}
p_{ij}(x_i, x_j) = \sum_{x_1} \cdots \sum_{ \highlight{ x_{i - 1} } } \sum_{ \highlight{ x_{i + 1} } } \cdots \sum_{ \highlight{ x_{j - 1} } } \sum_{ \highlight{ x_{j + 1} } } \cdots \sum_{x_n} p(x_1, x_2, \dots x_n)
\end{split}
\end{equation}

Also, the following equalities for marginal probabilities -- are obvious (keeping in mind Equation \eqref{eq:probability_sum}):
\begin{equation} \label{eq:sum_over_states_of_one_node_marginal_probabilities}
\begin{split}
\highlightred{ \sum_{x_i} p_i(x_i) } & =  \sum_{ \highlight{ x_i } } \left( \sum_{x_1} \cdots \sum_{ \highlight{ x_{i - 1} } } \sum_{ \highlight{ x_{i + 1} } } \cdots \sum_{x_n} p(x_1, x_2, \dots x_n) \right) = \\
                               & = \sum_{x_1} \sum_{x_2} \cdots \sum_{x_n}  p(x_1, x_2, \dots x_n) \highlightred{ = 1 } \\
\highlightred{ \sum_{x_j} p_j(x_j) } & = \sum_{ \highlight{ x_j } } \left( \sum_{x_1} \cdots \sum_{ \highlight{ x_{j - 1} } } \sum_{ \highlight{ x_{j + 1} } } \cdots \sum_{x_n} p(x_1, x_2, \dots x_n) \right) = \\
                               & = \sum_{x_1} \sum_{x_2} \cdots \sum_{x_n}  p(x_1, x_2, \dots x_n) \highlightred{ = 1 }
\end{split}
\end{equation}

\begin{equation} \label{eq:sum_over_states_of_two_node_marginal_probabilities}
\begin{split}
\highlightred{ \sum_{x_i} \sum_{x_j} p_{ij}(x_i, x_j) } & = \sum_{ \highlight{ x_i } } \sum_{ \highlight{ x_j } } \left( \sum_{x_1} \cdots \sum_{ \highlight{ x_{i - 1} } } \sum_{ \highlight{ x_{i + 1} } } \cdots \sum_{ \highlight{ x_{j - 1} } } \sum_{ \highlight{ x_{j + 1} } } \cdots \sum_{x_n} p(x_1, x_2, \dots x_n) \right) = \\
                                                           & = \sum_{x_1} \sum_{x_2} \cdots \sum_{x_n}  p(x_1, x_2, \dots x_n) \highlightred{ = 1 } \\
\end{split}
\end{equation}

Lets represent single-node marginal probabilities, using pairwise marginal probabilities (keeping in mind Equation \eqref{eq:marginal_probabilities_two_nodes}):
\begin{equation} \label{eq:one_node_marginal_probabilities_from_two_node_marginal_probabilities}
\begin{split}
\highlightred{ p_i(x_i) } & = \sum_{x_1} \cdots \sum_{ x_{i - 1} } \sum_{ x_{i + 1} } \cdots \sum_{x_n} p(x_1, x_2, \dots x_n) = \\
             & = \sum_{\highlight{x_j}} \left( \sum_{x_1} \cdots \sum_{ x_{i - 1} } \sum_{ x_{i + 1} } \cdots \sum_{ \highlight{x_{j - 1}} } \sum_{ \highlight{x_{j + 1}} } \cdots \sum_{x_n} p(x_1, x_2, \dots x_n) \right) = \\
             & \highlightred{ = \sum_{x_j} p_{ij}(x_i, x_j) } \\
\highlightred{ p_j(x_j) } & = \sum_{x_1} \cdots \sum_{ x_{j - 1} } \sum_{ x_{j + 1} } \cdots \sum_{x_n} p(x_1, x_2, \dots x_n) = \\
             & = \sum_{\highlight{x_i}} \left( \sum_{x_1} \cdots \sum_{ \highlight{x_{i - 1}} } \sum_{ \highlight{x_{i + 1}} } \cdots \sum_{ x_{j - 1} } \sum_{ x_{j + 1} } \cdots \sum_{x_n} p(x_1, x_2, \dots x_n) \right) = \\
             & \highlightred{ = \sum_{x_i} p_{ij}(x_i, x_j) }
\end{split}
\end{equation}

Lets consider summation over all edges - of some functions $f_i(x_i)$ and $f_j(x_j)$ (these functions depends only on states of single nodes: $i$ and $j$), also lets denote $q_i$ -- as a number of edges, which connected to node $i$ (\lq \lq degree\rq \rq\ of node $i$), and $q_j$ -- \lq \lq degree\rq \rq\ of node $j$. So, in this case -- we can represent summations over all edges, as summations over nodes, multiplied by node-degrees:
\begin{equation} \label{eq:sum_over_edges_to_sum_over_nodes}
\begin{split}
& \sum_{(i,j)} f_i(x_i) = \sum_{i} \left( q_i \times f_i(x_i) \right) \\
& \sum_{(i,j)} f_j(x_j) = \sum_{j} \left( q_j \times f_j(x_j) \right)
\end{split}
\end{equation}

Hammersley-Clifford Theorem \cite{hammersley_clifford_original, hammersley_clifford_proof, wikipedia_hammersley_clifford} states, that probability distribution of configurations of Markov Random Field can be factorized into product of non-negative functions (\lq \lq potentials\rq \rq) -- defined over maximal cliques of graph:
\begin{equation} \label{eq:hammersley_clifford_probability}
p(x_1, x_2, \dots x_n) = {1 \over Z} \times \left( \prod_{(i, j)} \psi_{ij}(x_i, x_j) \right) \times \left( \prod_{i} \phi_i(x_i) \right) \times \left( \prod_{j} \phi_j(x_j) \right)
\end{equation}

Boltzmann's Law:
\begin{equation*}
p(x_1, x_2, \dots x_n) = {1 \over Z} \times e^{{-E(x_1, x_2, \dots x_n) \over T}}
\end{equation*}

We can treat $T$ just as a scaling coefficient, and for simplicity lets assume that $T = 1$:
\begin{equation} \label{eq:boltzmanns_probability_t_1}
p(x_1, x_2, \dots x_n) = {1 \over Z} \times e^{-E(x_1, x_2, \dots x_n)}
\end{equation}

From \eqref{eq:hammersley_clifford_probability} and \eqref{eq:boltzmanns_probability_t_1}:
\begin{equation*}
e^{-E(x_1, x_2, \dots x_n)} = \left( \prod_{(i, j)} \psi_{ij}(x_i, x_j) \right) \times \left( \prod_{i} \phi_i(x_i) \right) \times \left( \prod_{j} \phi_j(x_j) \right)
\end{equation*}

So:
\begin{equation} \label{eq:energy_expression}
E(x_1, x_2, \dots x_n) = - \left( \sum_{(i, j)} ln(\psi_{ij}(x_i, x_j)) \right) - \left( \sum_{i} ln(\phi_i(x_i)) \right) - \left( \sum_{j} ln(\phi_j(x_j)) \right)
\end{equation}

Lets, assume that we have function $b(x_1, x_2, \dots x_n)$, which approximates real probability density function $p(x_1, x_2, \dots x_n)$.
It means, that \Cref{eq:probability_sum,eq:marginal_probabilities_one_node,eq:marginal_probabilities_two_nodes,eq:sum_over_states_of_one_node_marginal_probabilities,eq:sum_over_states_of_two_node_marginal_probabilities,eq:one_node_marginal_probabilities_from_two_node_marginal_probabilities} the same for $b(x_1, x_2, \dots x_n)$:

\begin{equation} \label{eq:probability_sum_b}
\sum_{x_1} \sum_{x_2} \cdots \sum_{x_n}  b(x_1, x_2, \dots x_n) = 1
\end{equation}

\begin{equation} \label{eq:marginal_probabilities_one_node_b}
\begin{split}
b_i(x_i) = \sum_{x_1} \cdots \sum_{ \highlight{ x_{i - 1} } } \sum_{ \highlight{ x_{i + 1} } } \cdots \sum_{x_n} b(x_1, x_2, \dots x_n) \\
b_j(x_i) = \sum_{x_1} \cdots \sum_{ \highlight{ x_{j - 1} } } \sum_{ \highlight{ x_{j + 1} } } \cdots \sum_{x_n} b(x_1, x_2, \dots x_n) \\
\end{split}
\end{equation}

\begin{equation} \label{eq:marginal_probabilities_two_nodes_b}
\begin{split}
b_{ij}(x_i, x_j) = \sum_{x_1} \cdots \sum_{ \highlight{ x_{i - 1} } } \sum_{ \highlight{ x_{i + 1} } } \cdots \sum_{ \highlight{ x_{j - 1} } } \sum_{ \highlight{ x_{j + 1} } } \cdots \sum_{x_n} b(x_1, x_2, \dots x_n)
\end{split}
\end{equation}

\begin{equation} \label{eq:sum_over_states_of_one_node_marginal_probabilities_b}
\begin{split}
\sum_{x_i} b_i(x_i) & = 1 \\
\sum_{x_j} b_j(x_j) & = 1
\end{split}
\end{equation}

\begin{equation} \label{eq:sum_over_states_of_two_node_marginal_probabilities_b}
\begin{split}
\sum_{x_i} \sum_{x_j} b_{ij}(x_i, x_j) = 1
\end{split}
\end{equation}

\begin{equation} \label{eq:one_node_marginal_probabilities_from_two_node_marginal_probabilities_b}
\begin{split}
b_i(x_i) & = \sum_{x_j} b_{ij}(x_i, x_j) \\
b_j(x_j) & = \sum_{x_i} b_{ij}(x_i, x_j) \\
\end{split}
\end{equation}

Lets use Kullback-Leibler divergence for measurement of \lq \lq difference\rq \rq\ between real and approximated functions:
\begin{equation}
\begin{split}
D_{KL}(b||p) = & \sum_{x_1} \sum_{x_2} \cdots \sum_{x_n} \left( b(x_1, x_2, \dots x_n) \times \highlight{ ln \left( {b(x_1, x_2, \dots x_n) \over p(x_1, x_2, \dots x_n)} \right) } \right) = \\
            = & \left( \sum_{x_1} \sum_{x_2} \cdots \sum_{x_n} b(x_1, x_2, \dots x_n) \times \highlight{ ln \left( b(x_1, x_2, \dots x_n) \right) } \right) - \\
             - & \left( \sum_{x_1} \sum_{x_2} \cdots \sum_{x_n} b(x_1, x_2, \dots x_n) \times \highlight{ ln \left( p(x_1, x_2, \dots x_n) \right) } \right)
\end{split}
\end{equation}

Lest substitute $p(x_1, x_2, \dots x_n)$ using expression from equation \eqref{eq:boltzmanns_probability_t_1}:
\begin{equation}
\begin{split}
D_{KL}(b||p) = & \left( \sum_{x_1} \sum_{x_2} \cdots \sum_{x_n} b(x_1, x_2, \dots x_n) \times ln \left( b(x_1, x_2, \dots x_n) \right) \right) - \\
             - & \left( \sum_{x_1} \sum_{x_2} \cdots \sum_{x_n} b(x_1, x_2, \dots x_n) \times \highlight{ ln \left( {1 \over Z} \times e^{-E(x_1, x_2, \dots x_n)} \right) } \right) = \\
            = & \left( \sum_{x_1} \sum_{x_2} \cdots \sum_{x_n} b(x_1, x_2, \dots x_n) \times ln \left( b(x_1, x_2, \dots x_n) \right) \right) - \\
             - & \left( \sum_{x_1} \sum_{x_2} \cdots \sum_{x_n} b(x_1, x_2, \dots x_n) \times \highlight{ \left( -ln(Z) -E(x_1, x_2, \dots x_n) \right) } \right) = \\
            = & \left( \sum_{x_1} \sum_{x_2} \cdots \sum_{x_n} b(x_1, x_2, \dots x_n) \times ln \left( b(x_1, x_2, \dots x_n) \right) \right) + \\
            + & \left( \sum_{x_1} \sum_{x_2} \cdots \sum_{x_n} b(x_1, x_2, \dots x_n) \times \highlight{ ln(Z) } \right) + \\
             + & \left( \sum_{x_1} \sum_{x_2} \cdots \sum_{x_n} b(x_1, x_2, \dots x_n) \times \highlight{ E(x_1, x_2, \dots x_n) } \right) \\
\end{split}
\end{equation}

As far, as $Z$ is just a constant -- it doesn't depend on $x_1, x_2 \dots x_n$, so we can move $ln(Z)$ out of summations:
\begin{equation}
\begin{split}
D_{KL}(b||p) = & \left( \sum_{x_1} \sum_{x_2} \cdots \sum_{x_n} b(x_1, x_2, \dots x_n) \times ln \left( b(x_1, x_2, \dots x_n) \right) \right) + \\
            + & \highlight{ \left( \sum_{x_1} \sum_{x_2} \cdots \sum_{x_n} b(x_1, x_2, \dots x_n) \right) \times ln(Z) } + \\
            + & \left( \sum_{x_1} \sum_{x_2} \cdots \sum_{x_n} b(x_1, x_2, \dots x_n) \times E(x_1, x_2, \dots x_n) \right) \\
\end{split}
\end{equation}

Taking into account equation \eqref{eq:probability_sum_b}, we can substitute multiplier near $ln(Z)$ -- to $1$:
\begin{equation}
\begin{split}
D_{KL}(b||p) = & \left( \sum_{x_1} \sum_{x_2} \cdots \sum_{x_n} b(x_1, x_2, \dots x_n) \times ln \left( b(x_1, x_2, \dots x_n) \right) \right) + \\
            + & \left( \sum_{x_1} \sum_{x_2} \cdots \sum_{x_n} b(x_1, x_2, \dots x_n) \times E(x_1, x_2, \dots x_n) \right) + \highlight{ ln(Z) }
\end{split}
\end{equation}

Lets use following notation:
\begin{equation} \label{eq:U_H_G_F}
\begin{split}
U(b(x_1,x_2,\dots x_n))  & = \sum_{x_1} \sum_{x_2} \cdots \sum_{x_n} \left( b(x_1, x_2, \dots x_n) \times E(x_1, x_2, \dots x_n) \right) \\
-H(b(x_1,x_2,\dots x_n)) & = \sum_{x_1} \sum_{x_2} \cdots \sum_{x_n} \left( b(x_1, x_2, \dots x_n) \times ln \left( b(x_1, x_2, \dots x_n) \right) \right) \\
G(b(x_1,x_2,\dots x_n))  & = U(b(x_1,x_2,\dots x_n)) - H(b(x_1,x_2,\dots x_n)) \\
-F(b(x_1,x_2,\dots x_n)) & = -F = ln(Z)
\end{split}
\end{equation}

So:
\begin{equation} \label{eq:kl_divergence_via_U_H_ln_Z}
D_{KL}(b||p) = G(b(x_1,x_2,\dots x_n)) - F = U(b(x_1,x_2,\dots x_n)) - H(b(x_1,x_2,\dots x_n)) + ln(Z)
\end{equation}

Let's transform $U(b(x_1,x_2,\dots x_n))$, using expression for energy \eqref{eq:energy_expression}:
\begin{equation}
\begin{split}
   & U(b(x_1,x_2,\dots x_n)) = \sum_{x_1} \sum_{x_2} \cdots \sum_{x_n} \left( b(x_1, x_2, \dots x_n) \times E(x_1, x_2, \dots x_n) \right) = \\
= & \sum_{x_1} \sum_{x_2} \cdots \sum_{x_n} \left( b(x_1, x_2, \dots x_n) \times \left( - \left( \sum_{(i, j)} ln(\psi_{ij}(x_i, x_j)) \right) - \left( \sum_{i} ln(\phi_i(x_i)) \right) - \left( \sum_{j} ln(\phi_j(x_j)) \right) \right) \right) = \\
= & -\left( \sum_{x_1} \sum_{x_2} \cdots \sum_{x_n} \left( b(x_1, x_2, \dots x_n) \times \sum_{(i, j)} ln(\psi_{ij}(x_i, x_j)) \right) \right) - \\ 
   & -\left( \sum_{x_1} \sum_{x_2} \cdots \sum_{x_n} \left( b(x_1, x_2, \dots x_n) \times \sum_{i} ln(\phi_{i}(x_i)) \right) \right) - \\ 
   & -\left( \sum_{x_1} \sum_{x_2} \cdots \sum_{x_n} \left( b(x_1, x_2, \dots x_n) \times \sum_{j} ln(\phi_{j}(x_j)) \right) \right) = \\
= & -\left( \sum_{(i, j)} \sum_{x_1} \sum_{x_2} \cdots \sum_{x_n} \left( b(x_1, x_2, \dots x_n) \times ln(\psi_{ij}(x_i, x_j)) \right) \right) - \\
   & -\left( \sum_{i} \sum_{x_1} \sum_{x_2} \cdots \sum_{x_n} \left( b(x_1, x_2, \dots x_n) \times ln(\phi_{i}(x_i)) \right) \right) - \\
   & -\left( \sum_{j} \sum_{x_1} \sum_{x_2} \cdots \sum_{x_n} \left( b(x_1, x_2, \dots x_n) \times ln(\phi_{j}(x_j)) \right) \right) = \\
= & -\left( \sum_{(i, j)} \sum_{\highlight{x_i}} \sum_{\highlight{x_j}} \sum_{x_1} \cdots \sum_{\highlight{x_{i-1}}} \sum_{\highlight{x_{i+1}}} \cdots \sum_{\highlight{x_{j-1}}} \sum_{\highlight{x_{j+1}}} \cdots \sum_{x_n} \left( b(x_1, x_2, \dots x_n) \times ln(\psi_{ij}(x_i, x_j)) \right) \right) - \\
   & -\left( \sum_{i} \sum_{\highlight{x_i}} \sum_{x_1} \cdots \sum_{\highlight{x_{i-1}}} \sum_{\highlight{x_{i+1}}} \cdots \sum_{x_n} \left( b(x_1, x_2, \dots x_n) \times ln(\phi_{i}(x_i)) \right) \right) - \\
   & -\left( \sum_{j} \sum_{\highlight{x_j}} \sum_{x_1} \cdots \sum_{\highlight{x_{j-1}}} \sum_{\highlight{x_{j+1}}} \cdots \sum_{x_n} \left( b(x_1, x_2, \dots x_n) \times ln(\phi_{j}(x_j)) \right) \right)
\end{split}
\end{equation}

As far, as $\psi_{ij}(x_i, x_j)$ -- depends only on $x_i$ and $x_j$, and $\phi_{i}(x_i)$ -- depends only on $x_i$, and $\phi_{j}(x_j)$ -- depends only on $x_j$ -- we could rewrite sums as:
\begin{equation}
\begin{split}
   & U(b(x_1,x_2,\dots x_n)) = -\left( \sum_{(i, j)} \sum_{x_i} \sum_{x_j} ln(\psi_{ij}(x_i, x_j)) \times \left( \highlight{\sum_{x_1} \cdots \sum_{x_{i-1}} \sum_{x_{i+1}} \cdots \sum_{x_{j-1}} \sum_{x_{j+1}} \cdots \sum_{x_n}  b(x_1, x_2, \dots x_n)} \right) \right) - \\
   & -\left( \sum_{i} \sum_{x_i} ln(\phi_{i}(x_i)) \times \left( \highlight{\sum_{x_1} \cdots \sum_{x_{i-1}} \sum_{x_{i+1}} \cdots \sum_{x_n} b(x_1, x_2, \dots x_n)} \right) \right) - \\
   & -\left( \sum_{j} \sum_{x_j} ln(\phi_{j}(x_j)) \times \left( \highlight{\sum_{x_1} \cdots \sum_{x_{j-1}} \sum_{x_{j+1}} \cdots \sum_{x_n} b(x_1, x_2, \dots x_n)} \right) \right)
\end{split}
\end{equation}

Taking into account \Cref{eq:marginal_probabilities_one_node_b,eq:marginal_probabilities_two_nodes_b}: we can substitute summations of probability density function -- to marginal probabilities:
\begin{equation}
\begin{split}
   & U(b(x_1,x_2,\dots x_n)) = -\left( \sum_{(i, j)} \sum_{x_i} \sum_{x_j} ln(\psi_{ij}(x_i, x_j)) \times \highlight{b_{ij}(x_i, x_j)} \right) - \\
   & -\left( \sum_{i} \sum_{x_i} ln(\phi_{i}(x_i)) \times \highlight{b_i(x_i)} \right) - \left( \sum_{j} \sum_{x_j} ln(\phi_{j}(x_j)) \times \highlight{b_j(x_j)} \right)
\end{split}
\end{equation}

Lets introduce following variables (\lq \lq local energies\rq \rq):
\begin{equation} \label{eq:local_energies}
\begin{split}
& E_i(x_i)    = -ln(\phi_i(x_i)) \\
& E_j(x_j)    = -ln(\phi_j(x_j)) \\
& E_{ij}(x_i, x_j) = -ln(\psi_{ij}(x_i, x_j)) -ln(\phi_i(x_i)) -ln(\phi_j(x_j))
\end{split}
\end{equation}

So, now we can express $ln(\phi_i(x_i))$, $ln(\phi_j(x_j))$ and $ln(\psi_{ij}(x_i, x_j))$ -- using \lq \lq local energies\rq \rq\ \eqref{eq:local_energies}:
\begin{equation} \label{eq:ln_potentials_as_local_energies}
\begin{split}
& ln(\phi_i(x_i))    = -E_i(x_i) \\
& ln(\phi_j(x_j))    = -E_j(x_j) \\
& ln(\psi_{ij}(x_i, x_j)) = -E_{ij}(x_i, x_j) + E_i(x_i) + E_j(x_j)
\end{split}
\end{equation}

Lets transform $U(b(x_1,x_2,\dots x_n))$ using Equations \eqref{eq:ln_potentials_as_local_energies}:
\begin{equation}
\begin{split}
   & U(b(x_1,x_2,\dots x_n)) = -\left( \sum_{(i, j)} \sum_{x_i} \sum_{x_j} b_{ij}(x_i, x_j) \times (-E_{ij}(x_i, x_j) + E_i(x_i) + E_j(x_j)) \right) - \\
   & -\left( \sum_{i} \sum_{x_i} b_i(x_i) \times (-E_i(x_i)) \right) - \left( \sum_{j} \sum_{x_j} b_j(x_j) \times (-E_j(x_j)) \right) = \\
   & = \left( \sum_{(i, j)} \sum_{x_i} \sum_{x_j} b_{ij}(x_i, x_j) \times E_{ij}(x_i, x_j) \right) - \left( \sum_{(i, j)} \sum_{\highlight{x_i}} \sum_{x_j} b_{ij}(x_i, x_j) \times \highlight{E_i(x_i)} \right) - \\
   & -\left( \sum_{(i, j)} \sum_{\highlight{x_j}} \sum_{x_i} b_{ij}(x_i, x_j) \times \highlight{E_j(x_j)} \right) + \left( \sum_{i} \sum_{x_i} b_i(x_i) \times E_i(x_i) \right) + \left( \sum_{j} \sum_{x_j} b_j(x_j) \times E_j(x_j) \right)
\end{split}
\end{equation}

Taking into account that $E_i(x_i)$ is not depends on $x_j$, and $E_j(x_j)$ is not depends on $x_i$ - we can move these multipliers out of corresponding summations:
\begin{equation}
\begin{split}
   & U(b(x_1,x_2,\dots x_n)) = \left( \sum_{(i, j)} \sum_{x_i} \sum_{x_j} b_{ij}(x_i, x_j) \times E_{ij}(x_i, x_j) \right) - \left( \sum_{(i, j)} \sum_{\highlight{x_i}} \highlight{E_i(x_i)} \times \highlightred{\sum_{x_j} b_{ij}(x_i, x_j)} \right) - \\
   & -\left( \sum_{(i, j)} \sum_{\highlight{x_j}} \highlight{E_j(x_j)} \times \highlightred{\sum_{x_i} b_{ij}(x_i, x_j)} \right) + \left( \sum_{i} \sum_{x_i} b_i(x_i) \times E_i(x_i) \right) + \left( \sum_{j} \sum_{x_j} b_j(x_j) \times E_j(x_j) \right)
\end{split}
\end{equation}

Taking into account \eqref{eq:one_node_marginal_probabilities_from_two_node_marginal_probabilities_b} -- we can substitute summations over two-node marginal probabilities -- to single-node marginal probabilities:

\begin{equation}
\begin{split}
   & U(b(x_1,x_2,\dots x_n)) = \left( \sum_{(i, j)} \sum_{x_i} \sum_{x_j} b_{ij}(x_i, x_j) \times E_{ij}(x_i, x_j) \right) - \left( \sum_{(i, j)} \sum_{x_i} E_i(x_i) \times \highlightred{b_i(x_i)} \right) - \\
   & -\left( \sum_{(i, j)} \sum_{x_j} E_j(x_j) \times \highlightred{b_j(x_j)} \right) + \left( \sum_{i} \sum_{x_i} b_i(x_i) \times E_i(x_i) \right) + \left( \sum_{j} \sum_{x_j} b_j(x_j) \times E_j(x_j) \right)
\end{split}
\end{equation}

Finally, we can substitute summations over edges -- to summation over nodes, as described by Equation \eqref{eq:sum_over_edges_to_sum_over_nodes}:
\begin{equation}
\begin{split}
   & U(b(x_1,x_2,\dots x_n)) = \left( \sum_{(i, j)} \sum_{x_i} \sum_{x_j} b_{ij}(x_i, x_j) \times E_{ij}(x_i, x_j) \right) - \left( \sum_{x_i} \highlight{\sum_{(i, j)} E_i(x_i) \times b_i(x_i)} \right) - \\
   & -\left( \sum_{x_j} \highlight{\sum_{(i, j)} E_j(x_j) \times b_j(x_j)} \right) + \left( \sum_{i} \sum_{x_i} b_i(x_i) \times E_i(x_i) \right) + \left( \sum_{j} \sum_{x_j} b_j(x_j) \times E_j(x_j) \right) = \\
   & = \left( \sum_{(i, j)} \sum_{x_i} \sum_{x_j} b_{ij}(x_i, x_j) \times E_{ij}(x_i, x_j) \right) - \left( \sum_{x_i} \highlight{\sum_{i} q_i \times E_i(x_i) \times b_i(x_i)} \right) - \\
   & -\left( \sum_{x_j} \highlight{\sum_{j} q_j \times E_j(x_j) \times b_j(x_j)} \right) + \left( \sum_{i} \sum_{x_i} b_i(x_i) \times E_i(x_i) \right) + \left( \sum_{j} \sum_{x_j} b_j(x_j) \times E_j(x_j) \right) = \\
   & = \left( \sum_{(i, j)} \sum_{x_i} \sum_{x_j} b_{ij}(x_i, x_j) \times E_{ij}(x_i, x_j) \right) - \\ 
   & -\left( \sum_{i} \sum_{x_i} (q_i - 1) \times E_i(x_i) \times b_i(x_i) \right) -\left( \sum_{j} \sum_{x_j} (q_j - 1) \times E_j(x_j) \times b_j(x_j) \right)
\end{split}
\end{equation}

So, finally, we derived representation of $U(b(x_1,x_2,\dots x_n))$ in terms of marginal probabilities and \lq \lq local energies\rq \rq:
\begin{equation} \label{eq:U_in_terms_of_local_marginals_and_energies}
\begin{split}
   & U(b(x_1,x_2,\dots x_n)) = \left( \sum_{(i, j)} \sum_{x_i} \sum_{x_j} b_{ij}(x_i, x_j) \times E_{ij}(x_i, x_j) \right) - \\
   & -\left( \sum_{i} \sum_{x_i} (q_i - 1) \times E_i(x_i) \times b_i(x_i) \right) -\left( \sum_{j} \sum_{x_j} (q_j - 1) \times E_j(x_j) \times b_j(x_j) \right)
\end{split}
\end{equation}

Now, lets transform $-H(b(x_1,x_2,\dots x_n))$ from Equations \eqref{eq:U_H_G_F}:
\begin{equation} \label{eq:entropy_joint_probability}
-H(b(x_1,x_2,\dots x_n)) = \sum_{x_1} \sum_{x_2} \cdots \sum_{x_n} \left( b(x_1, x_2, \dots x_n) \times ln \left( b(x_1, x_2, \dots x_n) \right) \right)
\end{equation}

Lets introduce additional statement \cite{bethe_free_energy_kikuch_approximations_and_bp} (which can be proven by induction \cite{a_montanari_inference_in_graphical_models}) -- probability distribution can be expressed via marginal probabilities (over max-cliques and single nodes), and node-degrees of graph. For Pairwise Markov Random Field this statement can be formalised as:
\begin{equation} \label{eq:joint_probability_via_marginals}
b(x_1, x_2, \dots x_n) = {\prod_{(i, j)} b_{ij}(x_i, x_j) \over \prod_i {b_i(x_i)}^{q_i - 1} \times \prod_j {b_j(x_j)}^{q_j - 1}}
\end{equation}

So, lets rewrite Equation \eqref{eq:entropy_joint_probability}, keeping in mind Equation \eqref{eq:joint_probability_via_marginals}:
\begin{equation}
\begin{split}
  & -H(b(x_1,x_2,\dots x_n)) = \sum_{x_1} \sum_{x_2} \cdots \sum_{x_n} \left( b(x_1, x_2, \dots x_n) \times ln \left( \highlight{b(x_1, x_2, \dots x_n)} \right) \right) = \\
  & = \sum_{x_1} \sum_{x_2} \cdots \sum_{x_n} \left( b(x_1, x_2, \dots x_n) \times ln \left( \highlight{{\prod_{(i, j)} b_{ij}(x_i, x_j) \over \prod_i {b_i(x_i)}^{q_i - 1} \times \prod_j {b_j(x_j)}^{q_j - 1}}} \right) \right) = \\
  & = \sum_{x_1} \sum_{x_2} \cdots \sum_{x_n} \left( b(x_1, x_2, \dots x_n) \times \left( \highlight{ ln \left( \prod_{(i, j)} b_{ij}(x_i, x_j) \right) - ln \left( \prod_i {b_i(x_i)}^{q_i - 1} \right) - ln \left( \prod_j {b_j(x_j)}^{q_j - 1} \right)} \right) \right) = \\
  & = \sum_{x_1} \sum_{x_2} \cdots \sum_{x_n} \left( b(x_1, x_2, \dots x_n) \times \left( \highlight{ \left( \sum_{(i, j)} ln \left( b_{ij}(x_i, x_j) \right) \right) - \left( \sum_i ln \left({b_i(x_i)}^{q_i - 1}\right) \right) - \left( \sum_j ln \left({b_j(x_j)}^{q_j - 1}\right) \right)} \right) \right) = \\
  & = \left( \sum_{x_1} \sum_{x_2} \cdots \sum_{x_n} b(x_1, x_2, \dots x_n) \times \sum_{(i, j)} ln \left( b_{ij}(x_i, x_j) \right) \right) - \\
  & -\left( \sum_{x_1} \sum_{x_2} \cdots \sum_{x_n} b(x_1, x_2, \dots x_n) \times \sum_i ln \left({b_i(x_i)}^{q_i - 1}\right) \right) - \left( \sum_{x_1} \sum_{x_2} \cdots \sum_{x_n} b(x_1, x_2, \dots x_n) \times \sum_j ln \left({b_j(x_j)}^{q_j - 1}\right) \right) = \\
  & = \left( \sum_{(i, j)} \sum_{x_1} \sum_{x_2} \cdots \sum_{x_n} b(x_1, x_2, \dots x_n) \times ln \left( b_{ij}(x_i, x_j) \right) \right) - \\
  & -\left( \sum_i \sum_{x_1} \sum_{x_2} \cdots \sum_{x_n} b(x_1, x_2, \dots x_n) \times ln \left({b_i(x_i)}^{q_i - 1}\right) \right) - \left( \sum_j \sum_{x_1} \sum_{x_2} \cdots \sum_{x_n} b(x_1, x_2, \dots x_n) \times ln \left({b_j(x_j)}^{q_j - 1}\right) \right)
\end{split}
\end{equation}

Lets rearrange sums, according to dependance of marginal probabilities:
\begin{equation}
\begin{split}
  & -H(b(x_1,x_2,\dots x_n)) = \left( \sum_{(i, j)} \sum_{\highlight{x_i}} \sum_{\highlight{x_j}} \sum_{x_1} \cdots \sum_{\highlight{x_{i-1}}} \sum_{\highlight{x_{i+1}}} \cdots \sum_{\highlight{x_{j-1}}} \sum_{\highlight{x_{j+1}}} \cdots \sum_{x_n} b(x_1, x_2, \dots x_n) \times \highlight{ln \left( b_{ij}(x_i, x_j) \right)} \right) - \\
  & -\left( \sum_i \sum_{\highlight{x_i}} \sum_{x_1} \cdots \sum_{\highlight{x_{i - 1}}} \sum_{\highlight{x_{i + 1}}} \cdots \sum_{x_n} b(x_1, x_2, \dots x_n) \times \highlight{ln \left({b_i(x_i)}^{q_i - 1}\right)} \right) - \\ 
  & -\left( \sum_j \sum_{\highlight{x_j}} \sum_{x_1} \cdots \sum_{\highlight{x_{j - 1}}} \sum_{\highlight{x_{j + 1}}} \cdots \sum_{x_n} b(x_1, x_2, \dots x_n) \times \highlight{ln \left({b_j(x_j)}^{q_j - 1}\right)} \right) = \\
  & = \left( \sum_{(i, j)} \sum_{\highlight{x_i}} \sum_{\highlight{x_j}} \highlight{ln \left( b_{ij}(x_i, x_j) \right)} \times \left( \sum_{x_1} \cdots \sum_{\highlight{x_{i-1}}} \sum_{\highlight{x_{i+1}}} \cdots \sum_{\highlight{x_{j-1}}} \sum_{\highlight{x_{j+1}}} \cdots \sum_{x_n} b(x_1, x_2, \dots x_n) \right) \right) - \\
  & -\left( \sum_i \sum_{\highlight{x_i}} \highlight{ln \left({b_i(x_i)}^{q_i - 1}\right)} \times \left( \sum_{x_1} \cdots \sum_{\highlight{x_{i - 1}}} \sum_{\highlight{x_{i + 1}}} \cdots \sum_{x_n} b(x_1, x_2, \dots x_n) \right) \right) - \\
  & -\left( \sum_j \sum_{\highlight{x_j}} \highlight{ln \left({b_j(x_j)}^{q_j - 1}\right)} \times \left( \sum_{x_1} \cdots \sum_{\highlight{x_{j - 1}}} \sum_{\highlight{x_{j + 1}}} \cdots \sum_{x_n} b(x_1, x_2, \dots x_n) \right) \right)
\end{split}
\end{equation}

Again, taking into account \Cref{eq:marginal_probabilities_one_node_b,eq:marginal_probabilities_two_nodes_b}: we can substitute summations of probability density function -- to marginal probabilities:
\begin{equation}
\begin{split}
  & -H(b(x_1,x_2,\dots x_n)) = \left( \sum_{(i, j)} \sum_{x_i} \sum_{x_j} ln \left( b_{ij}(x_i, x_j) \right) \times \left( \highlight{ \sum_{x_1} \cdots \sum_{x_{i-1}} \sum_{x_{i+1}} \cdots \sum_{x_{j-1}} \sum_{x_{j+1}} \cdots \sum_{x_n} b(x_1, x_2, \dots x_n) } \right) \right) - \\
  & -\left( \sum_i \sum_{x_i} ln \left({b_i(x_i)}^{q_i - 1}\right) \times \left( \highlight{ \sum_{x_1} \cdots \sum_{x_{i - 1}} \sum_{x_{i + 1}} \cdots \sum_{x_n} b(x_1, x_2, \dots x_n) } \right) \right) - \\
  & -\left( \sum_j \sum_{x_j} ln \left({b_j(x_j)}^{q_j - 1}\right) \times \left( \highlight{ \sum_{x_1} \cdots \sum_{x_{j - 1}} \sum_{x_{j + 1}} \cdots \sum_{x_n} b(x_1, x_2, \dots x_n) } \right) \right) = \\
  & = \left( \sum_{(i, j)} \sum_{x_i} \sum_{x_j} ln \left( b_{ij}(x_i, x_j) \right) \times \highlight{ b_{ij}(x_i, x_j) } \right) - \\
  & -\left( \sum_i \sum_{x_i} ln \left({b_i(x_i)}^{q_i - 1}\right) \times \highlight{ b_i(x_i) } \right) - \left( \sum_j \sum_{x_j} ln \left({b_j(x_j)}^{q_j - 1}\right) \times \highlight{ b_j(x_j) } \right)
\end{split}
\end{equation}

So, finally, we can represent $-H(b(x_1,x_2,\dots x_n))$ only in terms of marginal probabilities:
\begin{equation} \label{eq:H_in_terms_of_local_marginals}
\begin{split}
  & -H(b(x_1,x_2,\dots x_n)) = \left( \sum_{(i, j)} \sum_{x_i} \sum_{x_j} ln \left( b_{ij}(x_i, x_j) \right) \times b_{ij}(x_i, x_j) \right) - \\
  & -\left( \sum_i \sum_{x_i} (q_i - 1) \times ln(b_i(x_i)) \times b_i(x_i) \right) - \left( \sum_j \sum_{x_j} (q_j - 1) \times ln(b_j(x_j)) \times b_j(x_j) \right)
\end{split}
\end{equation}

Lets represent Kullback-Leibler divergence \eqref{eq:kl_divergence_via_U_H_ln_Z} -- in terms of marginal probabilities and local energies (keeping in mind Equations \eqref{eq:U_in_terms_of_local_marginals_and_energies} and \eqref{eq:H_in_terms_of_local_marginals}):
\begin{equation}
\begin{split}
   & D_{KL}(b||p) = U(b(x_1,x_2,\dots x_n)) - H(b(x_1,x_2,\dots x_n)) + ln(Z) = \\
   & = \left( \sum_{(i, j)} \sum_{x_i} \sum_{x_j} b_{ij}(x_i, x_j) \times E_{ij}(x_i, x_j) \right) - \\
   & -\left( \sum_{i} \sum_{x_i} (q_i - 1) \times E_i(x_i) \times b_i(x_i) \right) -\left( \sum_{j} \sum_{x_j} (q_j - 1) \times E_j(x_j) \times b_j(x_j) \right) + \\
   & +\left( \sum_{(i, j)} \sum_{x_i} \sum_{x_j} ln \left( b_{ij}(x_i, x_j) \right) \times b_{ij}(x_i, x_j) \right) - \\
   & -\left( \sum_i \sum_{x_i} (q_i - 1) \times ln(b_i(x_i)) \times b_i(x_i) \right) - \left( \sum_j \sum_{x_j} (q_j - 1) \times ln(b_j(x_j)) \times b_j(x_j) \right) + ln(Z) = \\
   & = \left( \sum_{(i, j)} \sum_{x_i} \sum_{x_j} b_{ij}(x_i, x_j) \times \left( E_{ij}(x_i, x_j) + ln \left( b_{ij}(x_i, x_j) \right) \right) \right) - \\
   & -\left( \sum_{i} \sum_{x_i} (q_i - 1) \times b_i(x_i) \times \left( E_i(x_i) + ln(b_i(x_i)) \right) \right) -\left( \sum_{j} \sum_{x_j} (q_j - 1) \times b_j(x_j) \times \left( E_j(x_j) + ln(b_j(x_j)) \right) \right) + ln(Z)
\end{split}
\end{equation}

So, again, I would like to pay attention on the fact -- that we avoided potentially exponential amount of operations, during calculation of Kullback-Leibler divergence:
\begin{equation} \label{eq:D_KL}
\begin{split}
   & D_{KL}(b||p) = \sum_{x_1} \sum_{x_2} \cdots \sum_{x_n} \left( b(x_1, x_2, \dots x_n) \times ln \left( {b(x_1, x_2, \dots x_n) \over p(x_1, x_2, \dots x_n)} \right) \right) = \\
   & = \left( \sum_{(i, j)} \sum_{x_i} \sum_{x_j} b_{ij}(x_i, x_j) \times \left( E_{ij}(x_i, x_j) + ln \left( b_{ij}(x_i, x_j) \right) \right) \right) - \\
   & -\left( \sum_{i} \sum_{x_i} (q_i - 1) \times b_i(x_i) \times \left( E_i(x_i) + ln(b_i(x_i)) \right) \right) -\left( \sum_{j} \sum_{x_j} (q_j - 1) \times b_j(x_j) \times \left( E_j(x_j) + ln(b_j(x_j)) \right) \right) + ln(Z)
\end{split}
\end{equation}

So, our goal is to find such values of $b_i(x_i)$, $b_j(x_j)$ and $b_{ij}(x_i,x_j)$ -- which leads to minimal value of $D_{KL}(b||p)$, with respect to conditions, defined by \Cref{eq:sum_over_states_of_one_node_marginal_probabilities_b,eq:sum_over_states_of_two_node_marginal_probabilities_b,eq:one_node_marginal_probabilities_from_two_node_marginal_probabilities_b}. 

Lets use Lagrange multipliers method \cite{wikipedia_lagrange_multipliers} for this purpose. Lets construct lagrangian, according to restrictions from \Cref{eq:sum_over_states_of_one_node_marginal_probabilities_b,eq:sum_over_states_of_two_node_marginal_probabilities_b,eq:one_node_marginal_probabilities_from_two_node_marginal_probabilities_b}:
\begin{equation} \label{eq:lagrangian}
\begin{split}
   & \mathcal{L} = D_{KL}(b||p) + \left( \sum_i \gamma_i \times \left( 1 - \sum_{x_i} b_i(x_i) \right) \right) + \left( \sum_j \gamma_j \times \left( 1 - \sum_{x_j} b_j(x_j) \right) \right) + \\
   & + \left( \sum_{(i,j)} \gamma_{ij} \times \left( 1 - \sum_{x_i} \sum_{x_j} b_{ij}(x_i,x_j) \right) \right) + \left( \sum_i \sum_{x_i} \sum_{j \in N(i)} \lambda_{ji}(x_i) \times \left( b_i(x_i) - \sum_{x_j} b_{ij}(x_i,x_j) \right) \right) + \\
   & + \left( \sum_j \sum_{x_j} \sum_{i \in N(j)} \lambda_{ij}(x_j) \times \left( b_j(x_j) - \sum_{x_i} b_{ij}(x_i,x_j) \right) \right)
\end{split}
\end{equation}
Where: 
\begin{itemize}
  \item 
    $\gamma_i$, $\gamma_j$, $\gamma_{ij}$, $\lambda_{ji}(x_i)$ and $\lambda_{ij}(x_j)$ -- lagrange multipliers (just some constants)
  \item
    $\sum_{j \in N(i)}$ -- sum over node-indices $j$, which are adjacent to node $i$ (\lq \lq neighbours\rq \rq) \\
    Which means, that $\forall j \in N(i)$ exists edge (which represented by tuple $(i,j)$)
  \item
    $\sum_{i \in N(j)}$ -- sum over node-indices $i$, which are adjacent to node $j$ \\
\end{itemize}

\noindent
So, $\mathcal{L}$ -- is a function, which depends on variables:
\begin{itemize}
  \item
    $b_i(x_i)$ -- defined $\forall i \in OddNodes$, and $\forall x_i \in StatesOfNode(i)$
  \item
    $b_j(x_j)$ -- defined $\forall j \in EvenNodes$, and $\forall x_j \in StatesOfNode(j)$
  \item
    $b_{ij}(x_i,x_j)$ -- defined $\forall (i,j) \in Edges$, and $\forall x_i \in StatesOfNode(i)$, and $\forall x_j \in StatesOfNode(j)$ \\
\end{itemize}

\noindent
By definition of Lagrange multipliers method, we are interested in stationary points of $\mathcal{L}$:

\begin{equation} \label{eq:partial_b_ij}
{\partial \mathcal{L} \over \partial b_{ij}(x_i,x_j)} = 0,\ (\forall (i,j) \in Edges,\ \forall x_i \in StatesOfNode(i),\ \forall x_j \in StatesOfNode(j))
\end{equation}

\begin{equation}
{\partial \mathcal{L} \over \partial b_i(x_i)} = 0,\ (\forall i \in OddNodes,\ \forall x_i \in StatesOfNode(i))
\end{equation}

\begin{equation}
{\partial \mathcal{L} \over \partial b_j(x_j)} = 0,\ (\forall j \in EvenNodes,\ \forall x_j \in StatesOfNode(j))
\end{equation}

Lets transform Equation \eqref{eq:partial_b_ij}, using Equation \eqref{eq:lagrangian}:
\begin{equation}
\setlength{\jot}{10pt}
\begin{split}
   & {\partial \mathcal{L} \over \partial b_{ij}(x_i,x_j)} =  {\partial \left( D_{KL}(b||p) \right) \over \partial b_{ij}(x_i, x_j)} + {\partial \left( - \highlight{ \gamma_{ij} } \times b_{ij}(x_i,x_j) \right) \over \partial b_{ij}(x_i, x_j)} + \\
   & + {\partial \left( - \highlightred{ \lambda_{ji}(x_i) } \times b_{ij}(x_i,x_j) \right) \over \partial b_{ij}(x_i, x_j)} + {\partial \left( - \highlightred{ \lambda_{ij}(x_j) } \times b_{ij}(x_i,x_j) \right) \over \partial b_{ij}(x_i, x_j)} = \\
   & = {\partial \left( D_{KL}(b||p) \right) \over \partial b_{ij}(x_i, x_j)} - \highlight{ \gamma_{ij} } - \highlightred{ \lambda_{ji}(x_i) } - \highlightred{ \lambda_{ij}(x_j) } = 0
\end{split}
\end{equation}

For the further transformation of partial derivative -- lets use expression for $D_{KL}(b||p)$ from Equation \eqref{eq:D_KL}:
\begin{equation} \label{eq:partial_b_ij_transformed}
\setlength{\jot}{10pt}
\begin{split}
   & {\partial \mathcal{L} \over \partial b_{ij}(x_i,x_j)} = \highlight{ {\partial \left( D_{KL}(b||p) \right) \over \partial b_{ij}(x_i, x_j)} } - \gamma_{ij} - \lambda_{ji}(x_i) - \lambda_{ij}(x_j) = \\
   & = \highlight{ {\partial \left( b_{ij}(x_i, x_j) \times \left( E_{ij}(x_i, x_j) + ln \left( b_{ij}(x_i, x_j) \right) \right) \right) \over \partial b_{ij}(x_i, x_j)} } - \gamma_{ij} - \lambda_{ji}(x_i) - \lambda_{ij}(x_j) = \\
   & = \highlight{ E_{ij}(x_i, x_j) } + \highlight{ {\partial \left( b_{ij}(x_i, x_j) \times ln \left( b_{ij}(x_i, x_j) \right) \right) \over \partial b_{ij}(x_i, x_j)} } - \gamma_{ij} - \lambda_{ji}(x_i) - \lambda_{ij}(x_j) = \\
   & = \highlight{ E_{ij}(x_i, x_j) } + \highlight{ ln \left( b_{ij}(x_i, x_j) \right) } + \highlight{ 1 } - \gamma_{ij} - \lambda_{ji}(x_i) - \lambda_{ij}(x_j) = 0
\end{split}
\end{equation}

So, finally, from Equation \eqref{eq:partial_b_ij_transformed} -- we can derive expression for $ln \left( b_{ij}(x_i, x_j) \right)$:
\begin{equation}
   ln \left( b_{ij}(x_i, x_j) \right) = \gamma_{ij} + \lambda_{ji}(x_i) + \lambda_{ij}(x_j) - E_{ij}(x_i, x_j) - 1
\end{equation}

\begin{thebibliography}{20}

\bibitem{understanding_bp}
  Jonathan S. Yedidia, William T. Freeman, and Yair Weiss,
  \emph{Understanding Belief Propagation and its Generalizations},
  Mitsubishi Electric Research Laboratories,
  TR2001-22, 
  November 2001,
  \url{http://www.merl.com/publications/docs/TR2001-22.pdf}

\bibitem{hammersley_clifford_original}
  J. M. Hammersley, P. Clifford,
  \emph{Markov fields on finite graphs and lattices},
  1971,
  \url{http://www.statslab.cam.ac.uk/~grg/books/hammfest/hamm-cliff.pdf}.

\bibitem{hammersley_clifford_proof}
  Samson Cheung,
  \emph{Proof of Hammersley-Clifford Theorem},
  February 3, 2008,
  \url{http://web.kaist.ac.kr/~kyomin/Fall09MRF/Hammersley-Clifford_Theorem.pdf}

\bibitem{wikipedia_hammersley_clifford}
  \url{http://en.wikipedia.org/wiki/Hammersley-Clifford_theorem}
  
\bibitem{bethe_free_energy_kikuch_approximations_and_bp}
  Jonathan S. Yedidia, William T. Freeman, and Yair Weiss,
  \emph{Bethe free energy, Kikuchi approximations, and belief propagation algorithms},
  Mitsubishi Electric Research Laboratories,
  TR-2001-16,
  May 2001,
  \url{http://www.merl.com/publications/docs/TR2001-16.pdf}
  
\bibitem{a_montanari_inference_in_graphical_models}
  Andrea Montanari,
  \emph{Lecture Notes for Stat 375 Inference in Graphical Models},
  Department of Electrical Engineering and Department of Statistics, Stanford University,
  February 21, 2011,
  (page 41),
  \url{http://web.stanford.edu/~montanar/TEACHING/Stat375/handouts/notes_stat375_1.pdf}
  
\bibitem{constructing_generalized_bp}
  Jonathan S. Yedidia, William T. Freeman, and Yair Weiss,
  \emph{Constructing Free Energy Approximations and Generalized Belief Propagation Algorithms},
  Mitsubishi Electric Research Laboratories,
  TR2004-040,
  May 2004,
  \url{http://www.merl.com/publications/docs/TR2004-040.pdf}
  
\bibitem{wikipedia_lagrange_multipliers}
  \url{http://en.wikipedia.org/wiki/Lagrange_multiplier}
  
\end{thebibliography}

\end{document}
