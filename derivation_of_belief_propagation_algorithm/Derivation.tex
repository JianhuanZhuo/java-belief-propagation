\documentclass[fleqn,leqno]{article}

% Defining page margins
\usepackage[top=1in, bottom=1.5in, left=1in, right=1in]{geometry}

% Ability to generate urls
\usepackage{hyperref}

\usepackage{mathtools}
\usepackage{amsmath}

% Pretty printing of references to multiple equations
\usepackage{cleveref}
\newcommand{\crefrangeconjunction}{--}

% argmin function
\DeclareMathOperator*{\argmin}{arg\,min}

% http://tex.stackexchange.com/questions/118410/highlight-terms-in-equation-mode
\usepackage{xcolor}
\newcommand{\highlight}[1]{\colorbox{green!10}{$\displaystyle#1$}}
\newcommand{\highlightred}[1]{\colorbox{olive!10}{$\displaystyle#1$}}

\begin{document}

\title{Detailed derivation of Belief Propagation algorithm \\ on Pairwise Markov Random Fields}
\author{Yurii Lahodiuk \\ yura.lagodiuk@gmail.com}
\date{}
\maketitle

By definition of probability density function:
\begin{equation} \label{eq:probability_sum}
\sum_{x_1} \sum_{x_2} \cdots \sum_{x_n}  p(x_1, x_2, \dots x_n) = 1
\end{equation}

We are interested in \lq \lq single-node\rq \rq\ marginal probabilities:
\begin{equation} \label{eq:marginal_probabilities_one_node}
\begin{split}
p_i(x_i) = \sum_{x_1} \cdots \sum_{ \highlight{ x_{i - 1} } } \sum_{ \highlight{ x_{i + 1} } } \cdots \sum_{x_n} p(x_1, x_2, \dots x_n) \\
p_j(x_i) = \sum_{x_1} \cdots \sum_{ \highlight{ x_{j - 1} } } \sum_{ \highlight{ x_{j + 1} } } \cdots \sum_{x_n} p(x_1, x_2, \dots x_n) \\
\end{split}
\end{equation}

For nodes, which are connected -- we, also interested in \lq \lq pairwise\rq \rq\ marginal probabilities:
\begin{equation} \label{eq:marginal_probabilities_two_nodes}
\begin{split}
p_{ij}(x_i, x_j) = \sum_{x_1} \cdots \sum_{ \highlight{ x_{i - 1} } } \sum_{ \highlight{ x_{i + 1} } } \cdots \sum_{ \highlight{ x_{j - 1} } } \sum_{ \highlight{ x_{j + 1} } } \cdots \sum_{x_n} p(x_1, x_2, \dots x_n)
\end{split}
\end{equation}

Also, the following equalities for marginal probabilities -- are obvious (keeping in mind Equation \eqref{eq:probability_sum}):
\begin{equation} \label{eq:sum_over_states_of_one_node_marginal_probabilities}
\begin{split}
\highlightred{ \sum_{x_i} p_i(x_i) } & =  \sum_{ \highlight{ x_i } } \left( \sum_{x_1} \cdots \sum_{ \highlight{ x_{i - 1} } } \sum_{ \highlight{ x_{i + 1} } } \cdots \sum_{x_n} p(x_1, x_2, \dots x_n) \right) = \\
                               & = \sum_{x_1} \sum_{x_2} \cdots \sum_{x_n}  p(x_1, x_2, \dots x_n) \highlightred{ = 1 } \\
\highlightred{ \sum_{x_j} p_j(x_j) } & = \sum_{ \highlight{ x_j } } \left( \sum_{x_1} \cdots \sum_{ \highlight{ x_{j - 1} } } \sum_{ \highlight{ x_{j + 1} } } \cdots \sum_{x_n} p(x_1, x_2, \dots x_n) \right) = \\
                               & = \sum_{x_1} \sum_{x_2} \cdots \sum_{x_n}  p(x_1, x_2, \dots x_n) \highlightred{ = 1 }
\end{split}
\end{equation}

\begin{equation} \label{eq:sum_over_states_of_two_node_marginal_probabilities}
\begin{split}
\highlightred{ \sum_{x_i} \sum_{x_j} p_{ij}(x_i, x_j) } & = \sum_{ \highlight{ x_i } } \sum_{ \highlight{ x_j } } \left( \sum_{x_1} \cdots \sum_{ \highlight{ x_{i - 1} } } \sum_{ \highlight{ x_{i + 1} } } \cdots \sum_{ \highlight{ x_{j - 1} } } \sum_{ \highlight{ x_{j + 1} } } \cdots \sum_{x_n} p(x_1, x_2, \dots x_n) \right) = \\
                                                           & = \sum_{x_1} \sum_{x_2} \cdots \sum_{x_n}  p(x_1, x_2, \dots x_n) \highlightred{ = 1 } \\
\end{split}
\end{equation}

Lets represent single-node marginal probabilities, using pairwise marginal probabilities (keeping in mind Equation \eqref{eq:marginal_probabilities_two_nodes}):
\begin{equation} \label{eq:one_node_marginal_probabilities_from_two_node_marginal_probabilities}
\begin{split}
\highlightred{ p_i(x_i) } & = \sum_{x_1} \cdots \sum_{ x_{i - 1} } \sum_{ x_{i + 1} } \cdots \sum_{x_n} p(x_1, x_2, \dots x_n) = \\
             & = \sum_{\highlight{x_j}} \left( \sum_{x_1} \cdots \sum_{ x_{i - 1} } \sum_{ x_{i + 1} } \cdots \sum_{ \highlight{x_{j - 1}} } \sum_{ \highlight{x_{j + 1}} } \cdots \sum_{x_n} p(x_1, x_2, \dots x_n) \right) = \\
             & \highlightred{ = \sum_{x_j} p_{ij}(x_i, x_j) } \\
\highlightred{ p_j(x_j) } & = \sum_{x_1} \cdots \sum_{ x_{j - 1} } \sum_{ x_{j + 1} } \cdots \sum_{x_n} p(x_1, x_2, \dots x_n) = \\
             & = \sum_{\highlight{x_i}} \left( \sum_{x_1} \cdots \sum_{ \highlight{x_{i - 1}} } \sum_{ \highlight{x_{i + 1}} } \cdots \sum_{ x_{j - 1} } \sum_{ x_{j + 1} } \cdots \sum_{x_n} p(x_1, x_2, \dots x_n) \right) = \\
             & \highlightred{ = \sum_{x_i} p_{ij}(x_i, x_j) }
\end{split}
\end{equation}

Lets consider summation over all edges - of some functions $f_i(x_i)$ and $f_j(x_j)$ (these functions depends only on states of single nodes: $i$ and $j$), also lets denote $q_i$ -- as a number of edges, which connected to node $i$ (\lq \lq degree\rq \rq\ of node $i$), and $q_j$ -- \lq \lq degree\rq \rq\ of node $j$. So, in this case -- we can represent summations over all edges, as summations over nodes, multiplied by node-degrees:
\begin{equation} \label{eq:sum_over_edges_to_sum_over_nodes}
\begin{split}
& \sum_{(i,j)} f_i(x_i) = \sum_{i} \left( q_i \times f_i(x_i) \right) \\
& \sum_{(i,j)} f_j(x_j) = \sum_{j} \left( q_j \times f_j(x_j) \right)
\end{split}
\end{equation}

Hammersley-Clifford Theorem \cite{hammersley_clifford_original, hammersley_clifford_proof, wikipedia_hammersley_clifford} states, that probability distribution of configurations of Markov Random Field can be factorized into product of non-negative functions (\lq \lq potentials\rq \rq) -- defined over maximal cliques of graph:
\begin{equation} \label{eq:hammersley_clifford_probability}
p(x_1, x_2, \dots x_n) = {1 \over Z} \times \left( \prod_{(i, j)} \psi_{ij}(x_i, x_j) \right) \times \left( \prod_{i} \phi_i(x_i) \right) \times \left( \prod_{j} \phi_j(x_j) \right)
\end{equation}

Boltzmann's Law:
\begin{equation*}
p(x_1, x_2, \dots x_n) = {1 \over Z} \times e^{{-E(x_1, x_2, \dots x_n) \over T}}
\end{equation*}

We can treat $T$ just as a scaling coefficient, and for simplicity lets assume that $T = 1$:
\begin{equation} \label{eq:boltzmanns_probability_t_1}
p(x_1, x_2, \dots x_n) = {1 \over Z} \times e^{-E(x_1, x_2, \dots x_n)}
\end{equation}

From \eqref{eq:hammersley_clifford_probability} and \eqref{eq:boltzmanns_probability_t_1}:
\begin{equation*}
e^{-E(x_1, x_2, \dots x_n)} = \left( \prod_{(i, j)} \psi_{ij}(x_i, x_j) \right) \times \left( \prod_{i} \phi_i(x_i) \right) \times \left( \prod_{j} \phi_j(x_j) \right)
\end{equation*}

So:
\begin{equation} \label{eq:energy_expression}
E(x_1, x_2, \dots x_n) = - \left( \sum_{(i, j)} ln(\psi_{ij}(x_i, x_j)) \right) - \left( \sum_{i} ln(\phi_i(x_i)) \right) - \left( \sum_{j} ln(\phi_j(x_j)) \right)
\end{equation}

Lets, assume that we have function $b(x_1, x_2, \dots x_n)$, which approximates real probability density function $p(x_1, x_2, \dots x_n)$.
It means, that \Cref{eq:probability_sum,eq:marginal_probabilities_one_node,eq:marginal_probabilities_two_nodes,eq:sum_over_states_of_one_node_marginal_probabilities,eq:sum_over_states_of_two_node_marginal_probabilities,eq:one_node_marginal_probabilities_from_two_node_marginal_probabilities} the same for $b(x_1, x_2, \dots x_n)$:

\begin{equation} \label{eq:probability_sum_b}
\sum_{x_1} \sum_{x_2} \cdots \sum_{x_n}  b(x_1, x_2, \dots x_n) = 1
\end{equation}

\begin{equation} \label{eq:marginal_probabilities_one_node_b}
\begin{split}
b_i(x_i) = \sum_{x_1} \cdots \sum_{ \highlight{ x_{i - 1} } } \sum_{ \highlight{ x_{i + 1} } } \cdots \sum_{x_n} b(x_1, x_2, \dots x_n) \\
b_j(x_i) = \sum_{x_1} \cdots \sum_{ \highlight{ x_{j - 1} } } \sum_{ \highlight{ x_{j + 1} } } \cdots \sum_{x_n} b(x_1, x_2, \dots x_n) \\
\end{split}
\end{equation}

\begin{equation} \label{eq:marginal_probabilities_two_nodes_b}
\begin{split}
b_{ij}(x_i, x_j) = \sum_{x_1} \cdots \sum_{ \highlight{ x_{i - 1} } } \sum_{ \highlight{ x_{i + 1} } } \cdots \sum_{ \highlight{ x_{j - 1} } } \sum_{ \highlight{ x_{j + 1} } } \cdots \sum_{x_n} b(x_1, x_2, \dots x_n)
\end{split}
\end{equation}

\begin{equation} \label{eq:sum_over_states_of_one_node_marginal_probabilities_b}
\begin{split}
\sum_{x_i} b_i(x_i) & = 1 \\
\sum_{x_j} b_j(x_j) & = 1
\end{split}
\end{equation}

\begin{equation} \label{eq:sum_over_states_of_two_node_marginal_probabilities_b}
\begin{split}
\sum_{x_i} \sum_{x_j} b_{ij}(x_i, x_j) = 1
\end{split}
\end{equation}

\begin{equation} \label{eq:one_node_marginal_probabilities_from_two_node_marginal_probabilities_b}
\begin{split}
b_i(x_i) & = \sum_{x_j} b_{ij}(x_i, x_j) \\
b_j(x_j) & = \sum_{x_i} b_{ij}(x_i, x_j) \\
\end{split}
\end{equation}

Lets use Kullback-Leibler divergence for measurement of \lq \lq difference\rq \rq\ between real and approximated functions:
\begin{equation}
\begin{split}
D(b||p) = & \sum_{x_1} \sum_{x_2} \cdots \sum_{x_n} \left( b(x_1, x_2, \dots x_n) \times \highlight{ ln \left( {b(x_1, x_2, \dots x_n) \over p(x_1, x_2, \dots x_n)} \right) } \right) = \\
            = & \left( \sum_{x_1} \sum_{x_2} \cdots \sum_{x_n} b(x_1, x_2, \dots x_n) \times \highlight{ ln \left( b(x_1, x_2, \dots x_n) \right) } \right) - \\
             - & \left( \sum_{x_1} \sum_{x_2} \cdots \sum_{x_n} b(x_1, x_2, \dots x_n) \times \highlight{ ln \left( p(x_1, x_2, \dots x_n) \right) } \right)
\end{split}
\end{equation}

Lest substitute $p(x_1, x_2, \dots x_n)$ using expression from equation \eqref{eq:boltzmanns_probability_t_1}:
\begin{equation}
\begin{split}
D(b||p) = & \left( \sum_{x_1} \sum_{x_2} \cdots \sum_{x_n} b(x_1, x_2, \dots x_n) \times ln \left( b(x_1, x_2, \dots x_n) \right) \right) - \\
             - & \left( \sum_{x_1} \sum_{x_2} \cdots \sum_{x_n} b(x_1, x_2, \dots x_n) \times \highlight{ ln \left( {1 \over Z} \times e^{-E(x_1, x_2, \dots x_n)} \right) } \right) = \\
            = & \left( \sum_{x_1} \sum_{x_2} \cdots \sum_{x_n} b(x_1, x_2, \dots x_n) \times ln \left( b(x_1, x_2, \dots x_n) \right) \right) - \\
             - & \left( \sum_{x_1} \sum_{x_2} \cdots \sum_{x_n} b(x_1, x_2, \dots x_n) \times \highlight{ \left( -ln(Z) -E(x_1, x_2, \dots x_n) \right) } \right) = \\
            = & \left( \sum_{x_1} \sum_{x_2} \cdots \sum_{x_n} b(x_1, x_2, \dots x_n) \times ln \left( b(x_1, x_2, \dots x_n) \right) \right) + \\
            + & \left( \sum_{x_1} \sum_{x_2} \cdots \sum_{x_n} b(x_1, x_2, \dots x_n) \times \highlight{ ln(Z) } \right) + \\
             + & \left( \sum_{x_1} \sum_{x_2} \cdots \sum_{x_n} b(x_1, x_2, \dots x_n) \times \highlight{ E(x_1, x_2, \dots x_n) } \right) \\
\end{split}
\end{equation}

As far, as $Z$ is just a constant -- it doesn't depend on $x_1, x_2 \dots x_n$, so we can move $ln(Z)$ out of summations:
\begin{equation}
\begin{split}
D(b||p) = & \left( \sum_{x_1} \sum_{x_2} \cdots \sum_{x_n} b(x_1, x_2, \dots x_n) \times ln \left( b(x_1, x_2, \dots x_n) \right) \right) + \\
            + & \highlight{ \left( \sum_{x_1} \sum_{x_2} \cdots \sum_{x_n} b(x_1, x_2, \dots x_n) \right) \times ln(Z) } + \\
            + & \left( \sum_{x_1} \sum_{x_2} \cdots \sum_{x_n} b(x_1, x_2, \dots x_n) \times E(x_1, x_2, \dots x_n) \right) \\
\end{split}
\end{equation}

Taking into account equation \eqref{eq:probability_sum_b}, we can substitute multiplier near $ln(Z)$ -- to $1$:
\begin{equation}
\begin{split}
D(b||p) = & \left( \sum_{x_1} \sum_{x_2} \cdots \sum_{x_n} b(x_1, x_2, \dots x_n) \times ln \left( b(x_1, x_2, \dots x_n) \right) \right) + \\
            + & \left( \sum_{x_1} \sum_{x_2} \cdots \sum_{x_n} b(x_1, x_2, \dots x_n) \times E(x_1, x_2, \dots x_n) \right) + \highlight{ ln(Z) }
\end{split}
\end{equation}

Lets use following notation:
\begin{equation}
\begin{split}
U(b(x_1,x_2,\dots x_n))  & = \sum_{x_1} \sum_{x_2} \cdots \sum_{x_n} \left( b(x_1, x_2, \dots x_n) \times E(x_1, x_2, \dots x_n) \right) \\
-H(b(x_1,x_2,\dots x_n)) & = \sum_{x_1} \sum_{x_2} \cdots \sum_{x_n} \left( b(x_1, x_2, \dots x_n) \times ln \left( b(x_1, x_2, \dots x_n) \right) \right) \\
G(b(x_1,x_2,\dots x_n))  & = U(b(x_1,x_2,\dots x_n)) - H(b(x_1,x_2,\dots x_n)) \\
-F(b(x_1,x_2,\dots x_n)) & = -F = ln(Z)
\end{split}
\end{equation}

So:
\begin{equation}
D(b||p) = G(b(x_1,x_2,\dots x_n)) - F = G(b(x_1,x_2,\dots x_n)) + ln(Z)
\end{equation}

Let's transform $U(b(x_1,x_2,\dots x_n))$, using expression for energy \eqref{eq:energy_expression}:
\begin{equation}
\begin{split}
   & U(b(x_1,x_2,\dots x_n)) = \sum_{x_1} \sum_{x_2} \cdots \sum_{x_n} \left( b(x_1, x_2, \dots x_n) \times E(x_1, x_2, \dots x_n) \right) = \\
= & \sum_{x_1} \sum_{x_2} \cdots \sum_{x_n} \left( b(x_1, x_2, \dots x_n) \times \left( - \left( \sum_{(i, j)} ln(\psi_{ij}(x_i, x_j)) \right) - \left( \sum_{i} ln(\phi_i(x_i)) \right) - \left( \sum_{j} ln(\phi_j(x_j)) \right) \right) \right) = \\
= & -\left( \sum_{x_1} \sum_{x_2} \cdots \sum_{x_n} \left( b(x_1, x_2, \dots x_n) \times \sum_{(i, j)} ln(\psi_{ij}(x_i, x_j)) \right) \right) - \\ 
   & -\left( \sum_{x_1} \sum_{x_2} \cdots \sum_{x_n} \left( b(x_1, x_2, \dots x_n) \times \sum_{i} ln(\phi_{i}(x_i)) \right) \right) - \\ 
   & -\left( \sum_{x_1} \sum_{x_2} \cdots \sum_{x_n} \left( b(x_1, x_2, \dots x_n) \times \sum_{j} ln(\phi_{j}(x_j)) \right) \right) = \\
= & -\left( \sum_{(i, j)} \sum_{x_1} \sum_{x_2} \cdots \sum_{x_n} \left( b(x_1, x_2, \dots x_n) \times ln(\psi_{ij}(x_i, x_j)) \right) \right) - \\
   & -\left( \sum_{i} \sum_{x_1} \sum_{x_2} \cdots \sum_{x_n} \left( b(x_1, x_2, \dots x_n) \times ln(\phi_{i}(x_i)) \right) \right) - \\
   & -\left( \sum_{j} \sum_{x_1} \sum_{x_2} \cdots \sum_{x_n} \left( b(x_1, x_2, \dots x_n) \times ln(\phi_{j}(x_j)) \right) \right) = \\
= & -\left( \sum_{(i, j)} \sum_{\highlight{x_i}} \sum_{\highlight{x_j}} \sum_{x_1} \cdots \sum_{\highlight{x_{i-1}}} \sum_{\highlight{x_{i+1}}} \cdots \sum_{\highlight{x_{j-1}}} \sum_{\highlight{x_{j+1}}} \cdots \sum_{x_n} \left( b(x_1, x_2, \dots x_n) \times ln(\psi_{ij}(x_i, x_j)) \right) \right) - \\
   & -\left( \sum_{i} \sum_{\highlight{x_i}} \sum_{x_1} \cdots \sum_{\highlight{x_{i-1}}} \sum_{\highlight{x_{i+1}}} \cdots \sum_{x_n} \left( b(x_1, x_2, \dots x_n) \times ln(\phi_{i}(x_i)) \right) \right) - \\
   & -\left( \sum_{j} \sum_{\highlight{x_j}} \sum_{x_1} \cdots \sum_{\highlight{x_{j-1}}} \sum_{\highlight{x_{j+1}}} \cdots \sum_{x_n} \left( b(x_1, x_2, \dots x_n) \times ln(\phi_{j}(x_j)) \right) \right)
\end{split}
\end{equation}

As far, as $\psi_{ij}(x_i, x_j)$ -- depends only on $x_i$ and $x_j$, and $\phi_{i}(x_i)$ -- depends only on $x_i$, and $\phi_{j}(x_j)$ -- depends only on $x_j$ -- we could rewrite sums as:
\begin{equation}
\begin{split}
   & U(b(x_1,x_2,\dots x_n)) = -\left( \sum_{(i, j)} \sum_{x_i} \sum_{x_j} ln(\psi_{ij}(x_i, x_j)) \times \left( \highlight{\sum_{x_1} \cdots \sum_{x_{i-1}} \sum_{x_{i+1}} \cdots \sum_{x_{j-1}} \sum_{x_{j+1}} \cdots \sum_{x_n}  b(x_1, x_2, \dots x_n)} \right) \right) - \\
   & -\left( \sum_{i} \sum_{x_i} ln(\phi_{i}(x_i)) \times \left( \highlight{\sum_{x_1} \cdots \sum_{x_{i-1}} \sum_{x_{i+1}} \cdots \sum_{x_n} b(x_1, x_2, \dots x_n)} \right) \right) - \\
   & -\left( \sum_{j} \sum_{x_j} ln(\phi_{j}(x_j)) \times \left( \highlight{\sum_{x_1} \cdots \sum_{x_{j-1}} \sum_{x_{j+1}} \cdots \sum_{x_n} b(x_1, x_2, \dots x_n)} \right) \right)
\end{split}
\end{equation}

Taking into account \Cref{eq:marginal_probabilities_one_node_b,eq:marginal_probabilities_two_nodes_b}: we can substitute summations of probability density function -- to marginal probabilities:
\begin{equation}
\begin{split}
   & U(b(x_1,x_2,\dots x_n)) = -\left( \sum_{(i, j)} \sum_{x_i} \sum_{x_j} ln(\psi_{ij}(x_i, x_j)) \times \highlight{b_{ij}(x_i, x_j)} \right) - \\
   & -\left( \sum_{i} \sum_{x_i} ln(\phi_{i}(x_i)) \times \highlight{b_i(x_i)} \right) - \left( \sum_{j} \sum_{x_j} ln(\phi_{j}(x_j)) \times \highlight{b_j(x_j)} \right)
\end{split}
\end{equation}

Lets introduce following variables (\lq \lq local energies\rq \rq):
\begin{equation} \label{eq:local_energies}
\begin{split}
& E_i(x_i)    = -ln(\phi_i(x_i)) \\
& E_j(x_j)    = -ln(\phi_j(x_j)) \\
& E_{ij}(x_i, x_j) = -ln(\psi_{ij}(x_i, x_j)) -ln(\phi_i(x_i)) -ln(\phi_j(x_j))
\end{split}
\end{equation}

So, now we can express $ln(\phi_i(x_i))$, $ln(\phi_j(x_j))$ and $ln(\psi_{ij}(x_i, x_j))$ -- using \lq \lq local energies\rq \rq\ \eqref{eq:local_energies}:
\begin{equation} \label{eq:ln_potentials_as_local_energies}
\begin{split}
& ln(\phi_i(x_i))    = -E_i(x_i) \\
& ln(\phi_j(x_j))    = -E_j(x_j) \\
& ln(\psi_{ij}(x_i, x_j)) = -E_{ij}(x_i, x_j) + E_i(x_i) + E_j(x_j)
\end{split}
\end{equation}

Lets transform $U(b(x_1,x_2,\dots x_n))$ using Equation \eqref{eq:ln_potentials_as_local_energies}:
\begin{equation}
\begin{split}
   & U(b(x_1,x_2,\dots x_n)) = -\left( \sum_{(i, j)} \sum_{x_i} \sum_{x_j} b_{ij}(x_i, x_j) \times (-E_{ij}(x_i, x_j) + E_i(x_i) + E_j(x_j)) \right) - \\
   & -\left( \sum_{i} \sum_{x_i} b_i(x_i) \times (-E_i(x_i)) \right) - \left( \sum_{j} \sum_{x_j} b_j(x_j) \times (-E_j(x_j)) \right) = \\
   & = \left( \sum_{(i, j)} \sum_{x_i} \sum_{x_j} b_{ij}(x_i, x_j) \times E_{ij}(x_i, x_j) \right) - \left( \sum_{(i, j)} \sum_{\highlight{x_i}} \sum_{x_j} b_{ij}(x_i, x_j) \times \highlight{E_i(x_i)} \right) - \\
   & -\left( \sum_{(i, j)} \sum_{\highlight{x_j}} \sum_{x_i} b_{ij}(x_i, x_j) \times \highlight{E_j(x_j)} \right) + \left( \sum_{i} \sum_{x_i} b_i(x_i) \times E_i(x_i) \right) + \left( \sum_{j} \sum_{x_j} b_j(x_j) \times E_j(x_j) \right)
\end{split}
\end{equation}

Taking into account that $E_i(x_i)$ is not depends on $x_j$, and $E_j(x_j)$ is not depends on $x_i$ - we can move these multipliers out of corresponding summations:
\begin{equation}
\begin{split}
   & U(b(x_1,x_2,\dots x_n)) = \left( \sum_{(i, j)} \sum_{x_i} \sum_{x_j} b_{ij}(x_i, x_j) \times E_{ij}(x_i, x_j) \right) - \left( \sum_{(i, j)} \sum_{\highlight{x_i}} \highlight{E_i(x_i)} \times \highlightred{\sum_{x_j} b_{ij}(x_i, x_j)} \right) - \\
   & -\left( \sum_{(i, j)} \sum_{\highlight{x_j}} \highlight{E_j(x_j)} \times \highlightred{\sum_{x_i} b_{ij}(x_i, x_j)} \right) + \left( \sum_{i} \sum_{x_i} b_i(x_i) \times E_i(x_i) \right) + \left( \sum_{j} \sum_{x_j} b_j(x_j) \times E_j(x_j) \right)
\end{split}
\end{equation}

Taking into account \eqref{eq:one_node_marginal_probabilities_from_two_node_marginal_probabilities_b} -- we can substitute summations over two-node marginal probabilities -- to single-node marginal probabilities:

\begin{equation}
\begin{split}
   & U(b(x_1,x_2,\dots x_n)) = \left( \sum_{(i, j)} \sum_{x_i} \sum_{x_j} b_{ij}(x_i, x_j) \times E_{ij}(x_i, x_j) \right) - \left( \sum_{(i, j)} \sum_{x_i} E_i(x_i) \times \highlightred{b_i(x_i)} \right) - \\
   & -\left( \sum_{(i, j)} \sum_{x_j} E_j(x_j) \times \highlightred{b_j(x_j)} \right) + \left( \sum_{i} \sum_{x_i} b_i(x_i) \times E_i(x_i) \right) + \left( \sum_{j} \sum_{x_j} b_j(x_j) \times E_j(x_j) \right)
\end{split}
\end{equation}

Finally, we can substitute summations over edges -- to summation over nodes, as described by Equation \eqref{eq:sum_over_edges_to_sum_over_nodes}:
\begin{equation}
\begin{split}
   & U(b(x_1,x_2,\dots x_n)) = \left( \sum_{(i, j)} \sum_{x_i} \sum_{x_j} b_{ij}(x_i, x_j) \times E_{ij}(x_i, x_j) \right) - \left( \sum_{x_i} \highlight{\sum_{(i, j)} E_i(x_i) \times b_i(x_i)} \right) - \\
   & -\left( \sum_{x_j} \highlight{\sum_{(i, j)} E_j(x_j) \times b_j(x_j)} \right) + \left( \sum_{i} \sum_{x_i} b_i(x_i) \times E_i(x_i) \right) + \left( \sum_{j} \sum_{x_j} b_j(x_j) \times E_j(x_j) \right) = \\
   & = \left( \sum_{(i, j)} \sum_{x_i} \sum_{x_j} b_{ij}(x_i, x_j) \times E_{ij}(x_i, x_j) \right) - \left( \sum_{x_i} \highlight{\sum_{i} q_i \times E_i(x_i) \times b_i(x_i)} \right) - \\
   & -\left( \sum_{x_j} \highlight{\sum_{j} q_j \times E_j(x_j) \times b_j(x_j)} \right) + \left( \sum_{i} \sum_{x_i} b_i(x_i) \times E_i(x_i) \right) + \left( \sum_{j} \sum_{x_j} b_j(x_j) \times E_j(x_j) \right) = \\
   & = \left( \sum_{(i, j)} \sum_{x_i} \sum_{x_j} b_{ij}(x_i, x_j) \times E_{ij}(x_i, x_j) \right) - \\ 
   & -\left( \sum_{i} \sum_{x_i} (q_i - 1) \times E_i(x_i) \times b_i(x_i) \right) -\left( \sum_{j} \sum_{x_j} (q_j - 1) \times E_j(x_j) \times b_j(x_j) \right)
\end{split}
\end{equation}

So, finally, we derived representation of $U(b(x_1,x_2,\dots x_n))$ in terms of marginal probabilities and \lq \lq local energies\rq \rq\:
\begin{equation}
\begin{split}
   & U(b(x_1,x_2,\dots x_n)) = \left( \sum_{(i, j)} \sum_{x_i} \sum_{x_j} b_{ij}(x_i, x_j) \times E_{ij}(x_i, x_j) \right) - \\
   & -\left( \sum_{i} \sum_{x_i} (q_i - 1) \times E_i(x_i) \times b_i(x_i) \right) -\left( \sum_{j} \sum_{x_j} (q_j - 1) \times E_j(x_j) \times b_j(x_j) \right)
\end{split}
\end{equation}

\begin{thebibliography}{9}

\bibitem{understanding_bp}
  Jonathan S. Yedidia, William T. Freeman, and Yair Weiss,
  \emph{Understanding Belief Propagation and its Generalizations},
  TR2001-22, 
  November 2001.

\bibitem{hammersley_clifford_original}
  J. M. Hammersley, P. Clifford,
  \emph{Markov fields on finite graphs and lattices},
  \url{http://www.statslab.cam.ac.uk/~grg/books/hammfest/hamm-cliff.pdf},
  1971.

\bibitem{hammersley_clifford_proof}
  Samson Cheung,
  \emph{Proof of Hammersley-Clifford Theorem},
  \url{http://web.kaist.ac.kr/~kyomin/Fall09MRF/Hammersley-Clifford_Theorem.pdf},
  February 3, 2008.

\bibitem{wikipedia_hammersley_clifford}
  \url{http://en.wikipedia.org/wiki/Hammersley-Clifford_theorem}.
  
\end{thebibliography}

\end{document}
